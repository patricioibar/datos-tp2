{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a80780",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is not supported\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:483)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1447)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# suprimir future warnings\u001b[39;00m\n\u001b[32m     10\u001b[39m warnings.simplefilter(action=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, category=\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m     11\u001b[39m spark = SparkSession.builder\\\n\u001b[32m     12\u001b[39m     .master(\u001b[33m\"\u001b[39m\u001b[33mlocal[*]\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     .getOrCreate()\n\u001b[32m     14\u001b[39m sc = spark.sparkContext\n\u001b[32m     15\u001b[39m sqlContext = SQLContext(sc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Patricio\\miniconda3\\envs\\jupyter_env\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = SparkContext.getOrCreate(sparkConf)\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Patricio\\miniconda3\\envs\\jupyter_env\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         SparkContext(conf=conf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Patricio\\miniconda3\\envs\\jupyter_env\\Lib\\site-packages\\pyspark\\core\\context.py:207\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    205\u001b[39m SparkContext._ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway=gateway, conf=conf)\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m    210\u001b[39m         sparkHome,\n\u001b[32m    211\u001b[39m         pyFiles,\n\u001b[32m    212\u001b[39m         environment,\n\u001b[32m    213\u001b[39m         batchSize,\n\u001b[32m    214\u001b[39m         serializer,\n\u001b[32m    215\u001b[39m         conf,\n\u001b[32m    216\u001b[39m         jsc,\n\u001b[32m    217\u001b[39m         profiler_cls,\n\u001b[32m    218\u001b[39m         udf_profiler_cls,\n\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28mself\u001b[39m.stop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Patricio\\miniconda3\\envs\\jupyter_env\\Lib\\site-packages\\pyspark\\core\\context.py:300\u001b[39m, in \u001b[36mSparkContext._do_init\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28mself\u001b[39m.environment[\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m] = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mPYTHONHASHSEED\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28mself\u001b[39m._jsc = jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._initialize_context(\u001b[38;5;28mself\u001b[39m._conf._jconf)\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[38;5;28mself\u001b[39m._conf = SparkConf(_jconf=\u001b[38;5;28mself\u001b[39m._jsc.sc().conf())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Patricio\\miniconda3\\envs\\jupyter_env\\Lib\\site-packages\\pyspark\\core\\context.py:429\u001b[39m, in \u001b[36mSparkContext._initialize_context\u001b[39m\u001b[34m(self, jconf)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[33;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm.JavaSparkContext(jconf)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Patricio\\miniconda3\\envs\\jupyter_env\\Lib\\site-packages\\py4j\\java_gateway.py:1627\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1621\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1622\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1623\u001b[39m     args_command +\\\n\u001b[32m   1624\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1626\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m return_value = get_return_value(\n\u001b[32m   1628\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m._gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m._fqn)\n\u001b[32m   1630\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1631\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Patricio\\miniconda3\\envs\\jupyter_env\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.UnsupportedOperationException: getSubject is not supported\r\n\tat java.base/javax.security.auth.Subject.getSubject(Subject.java:277)\r\n\tat org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:588)\r\n\tat org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2446)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2446)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:339)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)\r\n\tat java.base/jdk.internal.reflect.DirectConstructorHandleAccessor.newInstance(DirectConstructorHandleAccessor.java:62)\r\n\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:483)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1447)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "# suprimir future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f6215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_to_int(some_value):\n",
    "    if some_value is None:\n",
    "        return None\n",
    "    try:\n",
    "        return int(some_value)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407d095",
   "metadata": {},
   "source": [
    "# Consultas propias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd81ae",
   "metadata": {},
   "source": [
    "### 1. Monto total recaudado por ventas de los 5 productos con más reseñas positivas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd35545c",
   "metadata": {},
   "source": [
    "#### Hipótesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577dcfaa",
   "metadata": {},
   "source": [
    "- Se considera que una reseña es positiva cuando el rating de la misma es mayor que 3.\n",
    "- Si una reseña tiene un valor nulo en el *rating*, no se considera.\n",
    "- Si un *order_item* tiene valor nulo en *line_total*, el valor puede ser inferido a través del precio unitario y la cantidad comprada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883549d3",
   "metadata": {},
   "source": [
    "#### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_reviews_columns(row: Row):\n",
    "    rating = 0 if row.rating is None else row.rating\n",
    "    return (\n",
    "        parse_to_int(row.product_id),\n",
    "        rating,\n",
    "    )\n",
    "    \n",
    "reviewsIdx = {\n",
    "    \"product_id\": 0,\n",
    "    \"rating\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = sqlContext.read.csv(\n",
    "    'data/reviews.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "reviewsRDD = reviews.rdd.map(retain_reviews_columns).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a973849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_products_columns(row: Row):\n",
    "    product_name = \"UNDEFINED\" if row.product_name is None else row.product_name\n",
    "    brand = \"UNDEFINED\" if row.brand is None else row.brand\n",
    "    brand = brand.strip().upper()\n",
    "    return (\n",
    "        parse_to_int(row.product_id),\n",
    "        product_name,\n",
    "        brand,\n",
    "        parse_to_int(row.category_id),\n",
    "    )\n",
    "    \n",
    "productsIdx = {\n",
    "    \"id\": 0,\n",
    "    \"name\": 1,\n",
    "    \"brand\": 2,\n",
    "    \"category_id\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00030eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = sqlContext.read.csv(\n",
    "    'data/products.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "productsRDD = products.rdd.map(retain_products_columns).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_items_columns(row: Row):\n",
    "    qty = parse_to_int(row.quantity)\n",
    "    quantity = 0 if qty is None else qty\n",
    "    line_total = row.line_total if row.line_total is not None else infer_line_total(row)\n",
    "    return (\n",
    "        parse_to_int(row.product_id),\n",
    "        quantity,\n",
    "        line_total\n",
    "    )\n",
    "    \n",
    "def infer_line_total(row: Row):\n",
    "    u_price = parse_to_int(row.unit_price)\n",
    "    qty = parse_to_int(row.quantity)\n",
    "    if (\n",
    "        u_price is not None\n",
    "        and qty is not None\n",
    "    ):\n",
    "        return u_price * qty\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "itemsIdx = {\n",
    "    \"product_id\": 0,\n",
    "    \"quantity\": 1,\n",
    "    \"line_total\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d73be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = sqlContext.read.csv(\n",
    "    'data/order_items.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "itemsRDD = items.rdd.map(retain_items_columns).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9b33d",
   "metadata": {},
   "source": [
    "#### Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d42e306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 20:12:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, customer_id, product_id, rating, title, comment, is_verified_purchase, helpful_votes, created_at\n",
      " Schema: _c0, review_id, customer_id, product_id, rating, title, comment, is_verified_purchase, helpful_votes, created_at\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/reviews.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# no hay missing values en reviews.rating\n",
    "top_5_products = reviewsRDD.filter(lambda row: row[reviewsIdx[\"rating\"]] > 3) \\\n",
    "    .map(lambda row: (row[reviewsIdx[\"product_id\"]], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .takeOrdered(5, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6e6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 20:12:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , order_item_id, order_id, product_id, quantity, unit_price, line_total, discount_amount\n",
      " Schema: _c0, order_item_id, order_id, product_id, quantity, unit_price, line_total, discount_amount\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/order_items.csv\n"
     ]
    }
   ],
   "source": [
    "top_5_products_ids = [prod[0] for prod in top_5_products]\n",
    "top_5_products_sells = itemsRDD.filter(lambda row: row[itemsIdx[\"product_id\"]] in top_5_products_ids) \\\n",
    "    .map(lambda row: (row[itemsIdx[\"product_id\"]], row[itemsIdx[\"line_total\"]])) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65692e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 20:12:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , product_id, product_name, category_id, brand, price, cost, stock_quantity, weight_kg, dimensions, description, is_active, created_at\n",
      " Schema: _c0, product_id, product_name, category_id, brand, price, cost, stock_quantity, weight_kg, dimensions, description, is_active, created_at\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/products.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_5_products_names = productsRDD.filter(lambda row: row[productsIdx[\"id\"]] in top_5_products_ids) \\\n",
    "    .map(lambda row: (row[productsIdx[\"id\"]], row[productsIdx[\"name\"]])) \\\n",
    "    .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2732fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 productos con más reseñas positivas y monto de sus ventas totales:\n",
      "Producto Fully-configurable high-level circuit: $8320.50\n",
      "Producto Persevering logistical help-desk: $12220.00\n",
      "Producto Innovative solution-oriented installation: $291.20\n",
      "Producto Seamless radical architecture: $13547.28\n",
      "Producto Robust cohesive utilization: $271.51\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 productos con más reseñas positivas y monto de sus ventas totales:\")\n",
    "for prod in top_5_products_sells:\n",
    "    print(f\"Producto {top_5_products_names[prod[0]]}: ${prod[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d282cf",
   "metadata": {},
   "source": [
    "### 2. Durante 2024 ¿Qué porcentaje de las órdenes `REFUNDED` fueron órdenes con descuento? ¿La mayoría eran de usuarios activos? ¿Qué segmento de usuario realizó la mayor cantidad de reembolsos? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd32e654",
   "metadata": {},
   "source": [
    "#### Hipótesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff79a3",
   "metadata": {},
   "source": [
    "- Si el valor del campo *discount_amount* en *orders* es nulo, se asume que la órden no tuvo descuento.\n",
    "- Si el usuario de una órden no está en la tabla de *customers*, se asume que no es usuario activo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d25f83",
   "metadata": {},
   "source": [
    "#### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c0772",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m         status_dict[status] = \u001b[38;5;28mid\u001b[39m\n\u001b[32m      7\u001b[39m         \u001b[38;5;28mid\u001b[39m += \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m bc_status = sc.broadcast(status_dict)\n",
      "\u001b[31mNameError\u001b[39m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "status_dict = {}\n",
    "with open(\"status.txt\", \"r\") as f:\n",
    "    valid_statuses = [line.strip() for line in f.readlines()]\n",
    "    id = 0\n",
    "    for status in valid_statuses:\n",
    "        status_dict[status] = id\n",
    "        id += 1\n",
    "\n",
    "bc_status = sc.broadcast(status_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def retain_orders_columns(row: Row):\n",
    "    id = parse_to_int(row.customer_id)\n",
    "    datetime = get_orders_datetime(row)\n",
    "    year = datetime.year if datetime is not None else None\n",
    "    status_str = \"UNDEFINED\" if row.status is None else row.status.strip().upper()\n",
    "    status = status_dict.get(status_str, -1)\n",
    "    discount = 0.0 if row.discount_amount is None else row.discount_amount\n",
    "    return (\n",
    "        id,\n",
    "        discount,\n",
    "        status,\n",
    "        year,\n",
    "    )\n",
    "    \n",
    "def get_orders_datetime(row: Row):\n",
    "    return pd.to_datetime(row.order_date, format=\"%Y-%m-%dT%H:%M:%S.%f\", errors=\"coerce\")\n",
    "    \n",
    "ordersIdx = {\n",
    "    \"customer_id\": 0,\n",
    "    \"discount_amount\": 1,\n",
    "    \"status\": 2,\n",
    "    \"year\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067c9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders = sqlContext.read.csv(\n",
    "    'data/orders.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "ordersRDD = orders.rdd.map(retain_orders_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bebd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_customers_columns(row: Row):\n",
    "    id = parse_to_int(row.customer_id)\n",
    "    segment = \"UNDEFINED\" if row.customer_segment is None else row.customer_segment.strip().upper()\n",
    "    is_active = False if row.is_active is None else row.is_active\n",
    "    return (\n",
    "        id,\n",
    "        segment,\n",
    "        is_active,\n",
    "    )\n",
    "    \n",
    "customersIdx = {\n",
    "    \"id\": 0,\n",
    "    \"segment\": 1,\n",
    "    \"is_active\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = sqlContext.read.csv(\n",
    "    'data/customers.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "customersRDD = customers.rdd.map(retain_customers_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e8a81",
   "metadata": {},
   "source": [
    "#### Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0873b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_user_active_and_segment = ordersRDD \\\n",
    "    .filter(lambda row: row[ordersIdx[\"status\"]] == bc_status.value[\"REFUNDED\"] and row[ordersIdx[\"year\"]] == 2024) \\\n",
    "    .map(lambda row: (row[ordersIdx[\"customer_id\"]], row)) \\\n",
    "    .leftOuterJoin(customersRDD.map(lambda row: (row[customersIdx[\"id\"]], row))) \\\n",
    "    .map(lambda row: (\n",
    "        row[1][0][ordersIdx[\"discount_amount\"]],\n",
    "        row[1][0][ordersIdx[\"status\"]],\n",
    "        row[1][0][ordersIdx[\"year\"]],\n",
    "        row[1][1][customersIdx[\"segment\"]] if row[1][1] is not None else \"UNDEFINED\",\n",
    "        row[1][1][customersIdx[\"is_active\"]] if row[1][1] is not None else False,\n",
    "    )).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 20:12:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , order_id, customer_id, order_date, status, payment_method, shipping_address, billing_address, discount_amount, tax_amount, shipping_cost, total_amount, currency, created_at, updated_at, subtotal\n",
      " Schema: _c0, order_id, customer_id, order_date, status, payment_method, shipping_address, billing_address, discount_amount, tax_amount, shipping_cost, total_amount, currency, created_at, updated_at, subtotal\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/orders.csv\n",
      "25/10/05 20:13:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , customer_id, email, first_name, last_name, phone, date_of_birth, gender, country, city, postal_code, address, registration_date, last_login, is_active, customer_segment, marketing_consent\n",
      " Schema: _c0, customer_id, email, first_name, last_name, phone, date_of_birth, gender, country, city, postal_code, address, registration_date, last_login, is_active, customer_segment, marketing_consent\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/customers.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "discount_total_and_active_users = orders_user_active_and_segment \\\n",
    "    .map(\n",
    "        lambda row: (\n",
    "            1 if row[0] > 0 else 0, # tiene descuento\n",
    "            1 if row[4] else 0, # es de usuario activo\n",
    "            1,  # ordenes totales\n",
    "        )\n",
    "    ).reduce(lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2]))\n",
    "\n",
    "discount_refunded_orders_percentaje = (discount_total_and_active_users[0] / discount_total_and_active_users[2]) * 100\n",
    "active_user_percentaje = (discount_total_and_active_users[1] / discount_total_and_active_users[2]) * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a463af",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_refunded_segment = orders_user_active_and_segment \\\n",
    "    .map(lambda row: (row[3], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .reduce(lambda a, b: a if a[1] > b[1] else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476bb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.21286097052695"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_user_percentaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09045586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El 21.29% de las órdenes REFUNDED durante 2024 fueron órdenes con descuento.\n",
      "La mayoría eran de usuarios activos.\n",
      "El segmento que más órdenes REFUNDED tuvo fue REGULAR con 7284 órdenes.\n"
     ]
    }
   ],
   "source": [
    "print(f\"El {discount_refunded_orders_percentaje:.2f}% de las órdenes REFUNDED durante 2024 fueron órdenes con descuento.\")\n",
    "print(\"La mayoría eran de usuarios activos.\" if active_user_percentaje > 50 else \"La mayoría no eran de usuarios activos.\")\n",
    "print(f\"El segmento que más órdenes REFUNDED tuvo fue {most_refunded_segment[0]} con {most_refunded_segment[1]} órdenes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5413ad6",
   "metadata": {},
   "source": [
    "### 3. ¿Cuáles son las 3 marcas que vendieron menos unidades de productos durante 2025? Mostrar los nombres de los productos que más ingresos generaron de esas marcas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d7e43",
   "metadata": {},
   "source": [
    "#### Hipótesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500a3ab",
   "metadata": {},
   "source": [
    "- No se tienen en cuenta para este análisis las ventas cuyo producto no está registrado en la tabla de *products*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce440e39",
   "metadata": {},
   "source": [
    "#### Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafcb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_products_joined = itemsRDD \\\n",
    "    .map(\n",
    "        lambda row: (\n",
    "            row[itemsIdx[\"product_id\"]], \n",
    "            (row[itemsIdx[\"quantity\"]], row[itemsIdx[\"line_total\"]])\n",
    "        )\n",
    "    ).join(productsRDD.map( # como me interesa la información de marca, hago inner join\n",
    "        lambda row: (row[productsIdx[\"id\"]], (row[productsIdx[\"brand\"]]))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b702f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "less_sells_brands = items_products_joined.map(\n",
    "        lambda row: (\n",
    "            row[1][1], # brand\n",
    "            row[1][0][0], # quantity\n",
    "        )\n",
    "    ).reduceByKey(lambda a, b: a + b) \\\n",
    "    .takeOrdered(3, key=lambda x: x[1])\n",
    "less_sells_brands_names = [brand[0] for brand in less_sells_brands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_sells = itemsRDD.map(\n",
    "    lambda row: (row[itemsIdx[\"product_id\"]], row[itemsIdx[\"line_total\"]])\n",
    ").join(productsRDD.map(\n",
    "    lambda row: (row[productsIdx[\"id\"]], row[productsIdx[\"brand\"]]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef1684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_products_per_brand = brand_sells.filter(\n",
    "    lambda row: row[1][1] in less_sells_brands_names\n",
    ").map(\n",
    "    lambda row: ((row[1][1], row[0]), row[1][0])  # ((brand, product_id), line_total)\n",
    ").reduceByKey(lambda a, b: a + b) \\\n",
    ".map(\n",
    "    lambda row: (row[0][0], (row[0][1], row[1]))  # (brand, (product_id, total_line))\n",
    ").reduceByKey(lambda a, b: a if a[1] > b[1] else b) \\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products_per_brand_ids = [prod[1][0] for prod in top_products_per_brand]\n",
    "top_products_per_brand_names = productsRDD.filter(\n",
    "    lambda row: row[productsIdx[\"id\"]] in top_products_per_brand_ids\n",
    ").map(\n",
    "    lambda row: (row[productsIdx[\"id\"]], row[productsIdx[\"name\"]])\n",
    ").collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e0274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las marcas con menos unidades vendidas en 2025 son:\n",
      " - APPLE: 4942 unidades vendidas\n",
      " - ASHLEY FURNITURE: 5132 unidades vendidas\n",
      " - CASTROL: 5135 unidades vendidas\n",
      "\n",
      "El producto más vendido de cada una de esas marcas es:\n",
      " - CASTROL: Grass-roots directional success  con $83496.97 en ventas\n",
      " - ASHLEY FURNITURE: Distributed interactive neural-net con $30814.72 en ventas\n",
      " - APPLE: FACE-TO-FACE TANGIBLE STRATEGY con $133905.24 en ventas\n"
     ]
    }
   ],
   "source": [
    "print(\"Las marcas con menos unidades vendidas en 2025 son:\")\n",
    "for brand, amount in less_sells_brands:\n",
    "    print(f\" - {brand}: {amount} unidades vendidas\")\n",
    "    \n",
    "print(\"\\nEl producto más vendido de cada una de esas marcas es:\")\n",
    "for row in top_products_per_brand:\n",
    "    brand = row[0]\n",
    "    product_id = row[1][0]\n",
    "    total_line = row[1][1]\n",
    "    product_name = top_products_per_brand_names[product_id]\n",
    "    print(f\" - {brand}: {product_name} con ${total_line:.2f} en ventas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842df33e",
   "metadata": {},
   "source": [
    "### 4. Rating promedio de los productos pertenecientes a la categoría más vendida durante 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61494880",
   "metadata": {},
   "source": [
    "#### Hipótesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022ac5b",
   "metadata": {},
   "source": [
    "#### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_categories_columns(row: Row):\n",
    "    id = parse_to_int(row.category_id)\n",
    "    name = \"UNDEFINED\" if row.category_name is None else row.category_name.strip().upper()\n",
    "    parent = \"UNDEFINED\" if row.parent_category is None else row.parent_category.strip().upper()\n",
    "    return (\n",
    "        id,\n",
    "        name,\n",
    "        parent\n",
    "    )\n",
    "    \n",
    "categoriesIdx = {\n",
    "    \"id\": 0,\n",
    "    \"name\": 1,\n",
    "    \"parent\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230bf59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = sqlContext.read.csv(\n",
    "    'data/categories.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "categoriesRDD = categories.rdd.map(retain_categories_columns).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1d78a",
   "metadata": {},
   "source": [
    "#### Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fb3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_sells = itemsRDD.map(\n",
    "    lambda row: (row[itemsIdx[\"product_id\"]], row[itemsIdx[\"line_total\"]])\n",
    ").join(\n",
    "    productsRDD.filter(\n",
    "        lambda row: row[productsIdx[\"category_id\"]] is not None\n",
    "    ).map(\n",
    "        lambda row: (row[productsIdx[\"id\"]], row[productsIdx[\"category_id\"]])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a3806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_selled_category = category_sells.map(\n",
    "    lambda row: (row[1][1], row[1][0])  # (category_id, line_total)\n",
    ").reduceByKey(lambda a, b: a + b) \\\n",
    ".reduce(lambda a, b: a if a[1] > b[1] else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 20:14:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , category_id, category_name, parent_category, created_at\n",
      " Schema: _c0, category_id, category_name, parent_category, created_at\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/categories.csv\n"
     ]
    }
   ],
   "source": [
    "most_selled_category_name = categoriesRDD.filter(\n",
    "    lambda row: row[categoriesIdx[\"id\"]] == most_selled_category[0]\n",
    ").first()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_products = productsRDD.filter(\n",
    "        lambda row: (\n",
    "            row[productsIdx[\"category_id\"]] is not None\n",
    "            and row[productsIdx[\"category_id\"]] == most_selled_category[0]\n",
    "        )\n",
    "    ).map(\n",
    "        lambda row: row[productsIdx[\"id\"]]\n",
    "    ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695004bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "category_mean_rating = reviewsRDD.filter(\n",
    "    lambda row: row[reviewsIdx[\"product_id\"]] in category_products\n",
    ").map(\n",
    "    lambda row: row[reviewsIdx[\"rating\"]]\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2663565",
   "metadata": {},
   "source": [
    "### 5. Obtener los 3 productos `ELECTRONICS` con más movimientos por daños, y el promedio de cambios en la cantidad para los movimientos dañados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f7cb8",
   "metadata": {},
   "source": [
    "#### Hipótesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106ca8d",
   "metadata": {},
   "source": [
    "#### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73289f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_inventory_columns(row: Row):\n",
    "    product_id = parse_to_int(row.product_id)\n",
    "    reason = \"UNDEFINED\" if row.reason is None else row.reason.strip().upper()\n",
    "    quantity = parse_to_int(row.quantity_change)\n",
    "    return (\n",
    "        product_id,\n",
    "        reason,\n",
    "        0 if quantity is None else quantity,\n",
    "    )\n",
    "    \n",
    "inventoryIdx = {\n",
    "    \"product_id\": 0,\n",
    "    \"reason\": 1,\n",
    "    \"quantity\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory = sqlContext.read.csv(\n",
    "    'data/inventory_logs.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "inventoryRDD = inventory.rdd.map(retain_inventory_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a533cd53",
   "metadata": {},
   "source": [
    "#### Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "electronics_categories_id = categoriesRDD.filter(\n",
    "    lambda row: row[categoriesIdx[\"parent\"]] == \"ELECTRONICS\"\n",
    ").map(\n",
    "    lambda row: row[categoriesIdx[\"id\"]]\n",
    ").collect()\n",
    "\n",
    "electronics_products_ids = productsRDD.filter(\n",
    "    lambda row: (\n",
    "        row[productsIdx[\"category_id\"]] is not None\n",
    "        and row[productsIdx[\"category_id\"]] in electronics_categories_id\n",
    "    )\n",
    ").map(\n",
    "    lambda row: row[productsIdx[\"id\"]]\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a73c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_movements = inventoryRDD.filter(\n",
    "    lambda row: row[inventoryIdx[\"reason\"]] == \"DAMAGE\"\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_electronics_movements_by_product = damaged_movements \\\n",
    ".filter(lambda row: row[inventoryIdx[\"product_id\"]] in electronics_products_ids) \\\n",
    ".map(\n",
    "    lambda row:\n",
    "        (row[inventoryIdx[\"product_id\"]], (1, row[inventoryIdx[\"quantity\"]]))  # (product_id, (total, quantity_change))\n",
    ").reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798963c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/05 20:14:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , log_id, product_id, movement_type, quantity_change, reason, timestamp, reference_id, notes\n",
      " Schema: _c0, log_id, product_id, movement_type, quantity_change, reason, timestamp, reference_id, notes\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/inventory_logs.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_damaged_products = damaged_electronics_movements_by_product \\\n",
    "    .takeOrdered(3, key=lambda x: -x[1][0])\n",
    "\n",
    "most_damaged_products_ids = [prod[0] for prod in most_damaged_products]\n",
    "\n",
    "most_damaged_products_names = productsRDD.filter(\n",
    "    lambda row: row[productsIdx[\"id\"]] in most_damaged_products_ids\n",
    ").map(\n",
    "    lambda row: (row[productsIdx[\"id\"]], row[productsIdx[\"name\"]])\n",
    ").collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_movements_quantity_mean = damaged_movements.map(\n",
    "    lambda row: row[inventoryIdx[\"quantity\"]]\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f1a28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 3 productos ELECTRONICS con más movimientos por daños son:\n",
      "ID\tTotal Movs.\tCambio tot. en Cant.\tNombre\n",
      "909144\t4\t\t349\t\t\tSelf-enabling discrete open system\n",
      "986689\t4\t\t327\t\t\tUNDEFINED\n",
      "969306\t4\t\t-647\t\t\tPre-emptive zero tolerance encryption\n",
      "\n",
      "El promedio de cambios en la cantidad para los movimientos dañados es de -0.00 unidades.\n"
     ]
    }
   ],
   "source": [
    "print(\"Los 3 productos ELECTRONICS con más movimientos por daños son:\")\n",
    "print(\"ID\\tTotal Movs.\\tCambio tot. en Cant.\\tNombre\")\n",
    "for prod in most_damaged_products:\n",
    "    product_id = prod[0]\n",
    "    total_damages = prod[1][0]\n",
    "    quantity_change = prod[1][1]\n",
    "    product_name = most_damaged_products_names[product_id].strip()\n",
    "    print(f\"{product_id}\\t{total_damages}\\t\\t{quantity_change}\\t\\t\\t{product_name}\")\n",
    "    \n",
    "print(f\"\\nEl promedio de cambios en la cantidad para los movimientos dañados es de {damaged_movements_quantity_mean:.2f} unidades.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
