{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a80780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/06 12:54:47 WARN Utils: Your hostname, LENOVOID-ubuntu, resolves to a loopback address: 127.0.1.1; using 192.168.10.209 instead (on interface wlp2s0)\n",
      "25/10/06 12:54:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/06 12:54:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "# suprimir future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97f6215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_to_int(some_value):\n",
    "    if some_value is None:\n",
    "        return None\n",
    "    try:\n",
    "        return int(some_value)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407d095",
   "metadata": {},
   "source": [
    "# Consultas propias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd81ae",
   "metadata": {},
   "source": [
    "### 1. Monto total recaudado por ventas de los 5 productos con más reseñas positivas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd35545c",
   "metadata": {},
   "source": [
    "#### Hipótesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577dcfaa",
   "metadata": {},
   "source": [
    "- Se considera que una reseña es positiva cuando el rating de la misma es mayor que 3.\n",
    "- Si una reseña tiene un valor nulo en el *rating*, no se considera.\n",
    "- Si un *order_item* tiene valor nulo en *line_total*, el valor puede ser inferido a través del precio unitario y la cantidad comprada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883549d3",
   "metadata": {},
   "source": [
    "#### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac39861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_reviews_columns(row: Row):\n",
    "    rating = 0 if row.rating is None else row.rating\n",
    "    return (\n",
    "        parse_to_int(row.product_id),\n",
    "        rating,\n",
    "    )\n",
    "    \n",
    "reviewsIdx = {\n",
    "    \"product_id\": 0,\n",
    "    \"rating\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59d5c2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reviews = sqlContext.read.csv(\n",
    "    'data/reviews.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "reviewsRDD = reviews.rdd.map(retain_reviews_columns).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a973849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_products_columns(row: Row):\n",
    "    product_name = \"UNDEFINED\" if row.product_name is None else row.product_name\n",
    "    brand = \"UNDEFINED\" if row.brand is None else row.brand\n",
    "    brand = brand.strip().upper()\n",
    "    return (\n",
    "        parse_to_int(row.product_id),\n",
    "        product_name,\n",
    "        brand,\n",
    "        parse_to_int(row.category_id),\n",
    "    )\n",
    "    \n",
    "productsIdx = {\n",
    "    \"id\": 0,\n",
    "    \"name\": 1,\n",
    "    \"brand\": 2,\n",
    "    \"category_id\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00030eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "products = sqlContext.read.csv(\n",
    "    'data/products.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "productsRDD = products.rdd.map(retain_products_columns).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c85f8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_items_columns(row: Row):\n",
    "    qty = parse_to_int(row.quantity)\n",
    "    quantity = 0 if qty is None else qty\n",
    "    line_total = row.line_total if row.line_total is not None else infer_line_total(row)\n",
    "    return (\n",
    "        parse_to_int(row.product_id),\n",
    "        quantity,\n",
    "        line_total\n",
    "    )\n",
    "    \n",
    "def infer_line_total(row: Row):\n",
    "    u_price = parse_to_int(row.unit_price)\n",
    "    qty = parse_to_int(row.quantity)\n",
    "    if (\n",
    "        u_price is not None\n",
    "        and qty is not None\n",
    "    ):\n",
    "        return u_price * qty\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "itemsIdx = {\n",
    "    \"product_id\": 0,\n",
    "    \"quantity\": 1,\n",
    "    \"line_total\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d73be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = sqlContext.read.csv(\n",
    "    'data/order_items.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "itemsRDD = items.rdd.map(retain_items_columns).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9b33d",
   "metadata": {},
   "source": [
    "#### Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d42e306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/06 12:54:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , review_id, customer_id, product_id, rating, title, comment, is_verified_purchase, helpful_votes, created_at\n",
      " Schema: _c0, review_id, customer_id, product_id, rating, title, comment, is_verified_purchase, helpful_votes, created_at\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/reviews.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# no hay missing values en reviews.rating\n",
    "top_5_products = reviewsRDD.filter(lambda row: row[reviewsIdx[\"rating\"]] > 3) \\\n",
    "    .map(lambda row: (row[reviewsIdx[\"product_id\"]], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .takeOrdered(5, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06e6e6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/06 12:54:59 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , order_item_id, order_id, product_id, quantity, unit_price, line_total, discount_amount\n",
      " Schema: _c0, order_item_id, order_id, product_id, quantity, unit_price, line_total, discount_amount\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/order_items.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_5_products_ids = [prod[0] for prod in top_5_products]\n",
    "top_5_products_sells = itemsRDD.filter(lambda row: row[itemsIdx[\"product_id\"]] in top_5_products_ids) \\\n",
    "    .map(lambda row: (row[itemsIdx[\"product_id\"]], row[itemsIdx[\"line_total\"]])) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65692e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/06 12:55:00 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , product_id, product_name, category_id, brand, price, cost, stock_quantity, weight_kg, dimensions, description, is_active, created_at\n",
      " Schema: _c0, product_id, product_name, category_id, brand, price, cost, stock_quantity, weight_kg, dimensions, description, is_active, created_at\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/products.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_5_products_names = productsRDD.filter(lambda row: row[productsIdx[\"id\"]] in top_5_products_ids) \\\n",
    "    .map(lambda row: (row[productsIdx[\"id\"]], row[productsIdx[\"name\"]])) \\\n",
    "    .collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2732fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 productos con más reseñas positivas y monto de sus ventas totales:\n",
      "Producto Fully-configurable high-level circuit: $8320.50\n",
      "Producto Persevering logistical help-desk: $12220.00\n",
      "Producto Innovative solution-oriented installation: $291.20\n",
      "Producto Seamless radical architecture: $13547.28\n",
      "Producto Robust cohesive utilization: $271.51\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 productos con más reseñas positivas y monto de sus ventas totales:\")\n",
    "for prod in top_5_products_sells:\n",
    "    print(f\"Producto {top_5_products_names[prod[0]]}: ${prod[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d282cf",
   "metadata": {},
   "source": [
    "### 2. Durante 2024 ¿Qué porcentaje de las órdenes `REFUNDED` fueron órdenes con descuento? ¿La mayoría eran de usuarios activos? ¿Qué segmento de usuario realizó la mayor cantidad de reembolsos? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd32e654",
   "metadata": {},
   "source": [
    "#### Hipótesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff79a3",
   "metadata": {},
   "source": [
    "- Si el valor del campo *discount_amount* en *orders* es nulo, se asume que la órden no tuvo descuento.\n",
    "- Si el usuario de una órden no está en la tabla de *customers*, se asume que no es usuario activo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d25f83",
   "metadata": {},
   "source": [
    "#### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c0772",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_dict = {}\n",
    "with open(\"status.txt\", \"r\") as f:\n",
    "    valid_statuses = [line.strip() for line in f.readlines()]\n",
    "    id = 0\n",
    "    for status in valid_statuses:\n",
    "        status_dict[status] = id\n",
    "        id += 1\n",
    "\n",
    "bc_status = sc.broadcast(status_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d7c5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def retain_orders_columns(row: Row):\n",
    "    id = parse_to_int(row.customer_id)\n",
    "    datetime = get_orders_datetime(row)\n",
    "    year = datetime.year if datetime is not None else None\n",
    "    status_str = \"UNDEFINED\" if row.status is None else row.status.strip().upper()\n",
    "    status = status_dict.get(status_str, -1)\n",
    "    discount = 0.0 if row.discount_amount is None else row.discount_amount\n",
    "    return (\n",
    "        id,\n",
    "        discount,\n",
    "        status,\n",
    "        year,\n",
    "    )\n",
    "    \n",
    "def get_orders_datetime(row: Row):\n",
    "    return pd.to_datetime(row.order_date, format=\"%Y-%m-%dT%H:%M:%S.%f\", errors=\"coerce\")\n",
    "    \n",
    "ordersIdx = {\n",
    "    \"customer_id\": 0,\n",
    "    \"discount_amount\": 1,\n",
    "    \"status\": 2,\n",
    "    \"year\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a067c9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders = sqlContext.read.csv(\n",
    "    'data/orders.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "ordersRDD = orders.rdd.map(retain_orders_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53bebd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_customers_columns(row: Row):\n",
    "    id = parse_to_int(row.customer_id)\n",
    "    segment = \"UNDEFINED\" if row.customer_segment is None else row.customer_segment.strip().upper()\n",
    "    is_active = False if row.is_active is None else row.is_active\n",
    "    return (\n",
    "        id,\n",
    "        segment,\n",
    "        is_active,\n",
    "    )\n",
    "    \n",
    "customersIdx = {\n",
    "    \"id\": 0,\n",
    "    \"segment\": 1,\n",
    "    \"is_active\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d077924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = sqlContext.read.csv(\n",
    "    'data/customers.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "customersRDD = customers.rdd.map(retain_customers_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e8a81",
   "metadata": {},
   "source": [
    "#### Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0873b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_user_active_and_segment = ordersRDD \\\n",
    "    .filter(lambda row: row[ordersIdx[\"status\"]] == bc_status.value[\"REFUNDED\"] and row[ordersIdx[\"year\"]] == 2024) \\\n",
    "    .map(lambda row: (row[ordersIdx[\"customer_id\"]], row)) \\\n",
    "    .leftOuterJoin(customersRDD.map(lambda row: (row[customersIdx[\"id\"]], row))) \\\n",
    "    .map(lambda row: (\n",
    "        row[1][0][ordersIdx[\"discount_amount\"]],\n",
    "        row[1][1][customersIdx[\"segment\"]] if row[1][1] is not None else \"UNDEFINED\",\n",
    "        row[1][1][customersIdx[\"is_active\"]] if row[1][1] is not None else False,\n",
    "    )).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9660064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/06 12:57:50 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , order_id, customer_id, order_date, status, payment_method, shipping_address, billing_address, discount_amount, tax_amount, shipping_cost, total_amount, currency, created_at, updated_at, subtotal\n",
      " Schema: _c0, order_id, customer_id, order_date, status, payment_method, shipping_address, billing_address, discount_amount, tax_amount, shipping_cost, total_amount, currency, created_at, updated_at, subtotal\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/orders.csv\n",
      "25/10/06 12:59:05 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , customer_id, email, first_name, last_name, phone, date_of_birth, gender, country, city, postal_code, address, registration_date, last_login, is_active, customer_segment, marketing_consent\n",
      " Schema: _c0, customer_id, email, first_name, last_name, phone, date_of_birth, gender, country, city, postal_code, address, registration_date, last_login, is_active, customer_segment, marketing_consent\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/customers.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "discount_total_and_active_users = orders_user_active_and_segment \\\n",
    "    .map(\n",
    "        lambda row: (\n",
    "            1 if row[0] > 0 else 0, # tiene descuento\n",
    "            1 if row[2] else 0, # es de usuario activo\n",
    "            1,  # ordenes totales\n",
    "        )\n",
    "    ).reduce(lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2]))\n",
    "\n",
    "discount_refunded_orders_percentaje = (discount_total_and_active_users[0] / discount_total_and_active_users[2]) * 100\n",
    "active_user_percentaje = (discount_total_and_active_users[1] / discount_total_and_active_users[2]) * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "82a463af",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_refunded_segment = orders_user_active_and_segment \\\n",
    "    .map(lambda row: (row[1], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .reduce(lambda a, b: a if a[1] > b[1] else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d476bb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.21286097052695"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_user_percentaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09045586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El 21.29% de las órdenes REFUNDED durante 2024 fueron órdenes con descuento.\n",
      "La mayoría eran de usuarios activos.\n",
      "El segmento que más órdenes REFUNDED tuvo fue REGULAR con 7284 órdenes.\n"
     ]
    }
   ],
   "source": [
    "print(f\"El {discount_refunded_orders_percentaje:.2f}% de las órdenes REFUNDED durante 2024 fueron órdenes con descuento.\")\n",
    "print(\"La mayoría eran de usuarios activos.\" if active_user_percentaje > 50 else \"La mayoría no eran de usuarios activos.\")\n",
    "print(f\"El segmento que más órdenes REFUNDED tuvo fue {most_refunded_segment[0]} con {most_refunded_segment[1]} órdenes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5413ad6",
   "metadata": {},
   "source": [
    "### 3. ¿Cuáles son las 3 marcas que vendieron menos unidades de productos durante 2025? Mostrar los nombres de los productos que más ingresos generaron de esas marcas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77d7e43",
   "metadata": {},
   "source": [
    "#### Hipótesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500a3ab",
   "metadata": {},
   "source": [
    "- No se tienen en cuenta para este análisis las ventas cuyo producto no está registrado en la tabla de *products*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce440e39",
   "metadata": {},
   "source": [
    "#### Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fafcb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_products_joined = itemsRDD \\\n",
    "    .map(\n",
    "        lambda row: (\n",
    "            row[itemsIdx[\"product_id\"]], \n",
    "            (row[itemsIdx[\"quantity\"]], row[itemsIdx[\"line_total\"]])\n",
    "        )\n",
    "    ).join(productsRDD.map( # como me interesa la información de marca, hago inner join\n",
    "        lambda row: (row[productsIdx[\"id\"]], (row[productsIdx[\"brand\"]]))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a3b702f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "less_sells_brands = items_products_joined.map(\n",
    "        lambda row: (\n",
    "            row[1][1], # brand\n",
    "            row[1][0][0], # quantity\n",
    "        )\n",
    "    ).reduceByKey(lambda a, b: a + b) \\\n",
    "    .takeOrdered(3, key=lambda x: x[1])\n",
    "less_sells_brands_names = [brand[0] for brand in less_sells_brands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e18f2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_sells = itemsRDD.map(\n",
    "    lambda row: (row[itemsIdx[\"product_id\"]], row[itemsIdx[\"line_total\"]])\n",
    ").join(productsRDD.map(\n",
    "    lambda row: (row[productsIdx[\"id\"]], row[productsIdx[\"brand\"]]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ef1684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_products_per_brand = brand_sells.filter(\n",
    "    lambda row: row[1][1] in less_sells_brands_names\n",
    ").map(\n",
    "    lambda row: ((row[1][1], row[0]), row[1][0])  # ((brand, product_id), line_total)\n",
    ").reduceByKey(lambda a, b: a + b) \\\n",
    ".map(\n",
    "    lambda row: (row[0][0], (row[0][1], row[1]))  # (brand, (product_id, total_line))\n",
    ").reduceByKey(lambda a, b: a if a[1] > b[1] else b) \\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c146e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_products_per_brand_ids = [prod[1][0] for prod in top_products_per_brand]\n",
    "top_products_per_brand_names = productsRDD.filter(\n",
    "    lambda row: row[productsIdx[\"id\"]] in top_products_per_brand_ids\n",
    ").map(\n",
    "    lambda row: (row[productsIdx[\"id\"]], row[productsIdx[\"name\"]])\n",
    ").collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "564e0274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las marcas con menos unidades vendidas en 2025 son:\n",
      " - APPLE: 4942 unidades vendidas\n",
      " - ASHLEY FURNITURE: 5132 unidades vendidas\n",
      " - CASTROL: 5135 unidades vendidas\n",
      "\n",
      "El producto más vendido de cada una de esas marcas es:\n",
      " - CASTROL: Grass-roots directional success  con $83496.97 en ventas\n",
      " - ASHLEY FURNITURE: Distributed interactive neural-net con $30814.72 en ventas\n",
      " - APPLE: FACE-TO-FACE TANGIBLE STRATEGY con $133905.24 en ventas\n"
     ]
    }
   ],
   "source": [
    "print(\"Las marcas con menos unidades vendidas en 2025 son:\")\n",
    "for brand, amount in less_sells_brands:\n",
    "    print(f\" - {brand}: {amount} unidades vendidas\")\n",
    "    \n",
    "print(\"\\nEl producto más vendido de cada una de esas marcas es:\")\n",
    "for row in top_products_per_brand:\n",
    "    brand = row[0]\n",
    "    product_id = row[1][0]\n",
    "    total_line = row[1][1]\n",
    "    product_name = top_products_per_brand_names[product_id]\n",
    "    print(f\" - {brand}: {product_name} con ${total_line:.2f} en ventas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842df33e",
   "metadata": {},
   "source": [
    "### 4. Rating promedio de los productos pertenecientes a la categoría más vendida durante 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61494880",
   "metadata": {},
   "source": [
    "#### Hipótesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e022ac5b",
   "metadata": {},
   "source": [
    "#### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "528a0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_categories_columns(row: Row):\n",
    "    id = parse_to_int(row.category_id)\n",
    "    name = \"UNDEFINED\" if row.category_name is None else row.category_name.strip().upper()\n",
    "    parent = \"UNDEFINED\" if row.parent_category is None else row.parent_category.strip().upper()\n",
    "    return (\n",
    "        id,\n",
    "        name,\n",
    "        parent\n",
    "    )\n",
    "    \n",
    "categoriesIdx = {\n",
    "    \"id\": 0,\n",
    "    \"name\": 1,\n",
    "    \"parent\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "230bf59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = sqlContext.read.csv(\n",
    "    'data/categories.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "categoriesRDD = categories.rdd.map(retain_categories_columns).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1d78a",
   "metadata": {},
   "source": [
    "#### Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "545fb3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_sells = itemsRDD.map(\n",
    "    lambda row: (row[itemsIdx[\"product_id\"]], row[itemsIdx[\"line_total\"]])\n",
    ").join(\n",
    "    productsRDD.filter(\n",
    "        lambda row: row[productsIdx[\"category_id\"]] is not None\n",
    "    ).map(\n",
    "        lambda row: (row[productsIdx[\"id\"]], row[productsIdx[\"category_id\"]])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f5a3806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_selled_category = category_sells.map(\n",
    "    lambda row: (row[1][1], row[1][0])  # (category_id, line_total)\n",
    ").reduceByKey(lambda a, b: a + b) \\\n",
    ".reduce(lambda a, b: a if a[1] > b[1] else b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cab0e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/06 12:56:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , category_id, category_name, parent_category, created_at\n",
      " Schema: _c0, category_id, category_name, parent_category, created_at\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/categories.csv\n"
     ]
    }
   ],
   "source": [
    "most_selled_category_name = categoriesRDD.filter(\n",
    "    lambda row: row[categoriesIdx[\"id\"]] == most_selled_category[0]\n",
    ").first()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc2b103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_products = productsRDD.filter(\n",
    "        lambda row: (\n",
    "            row[productsIdx[\"category_id\"]] is not None\n",
    "            and row[productsIdx[\"category_id\"]] == most_selled_category[0]\n",
    "        )\n",
    "    ).map(\n",
    "        lambda row: row[productsIdx[\"id\"]]\n",
    "    ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "695004bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "category_mean_rating = reviewsRDD.filter(\n",
    "    lambda row: row[reviewsIdx[\"product_id\"]] in category_products\n",
    ").map(\n",
    "    lambda row: row[reviewsIdx[\"rating\"]]\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2663565",
   "metadata": {},
   "source": [
    "### 5. Obtener los 3 productos `ELECTRONICS` con más movimientos por daños, y el promedio de cambios en la cantidad para los movimientos dañados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f7cb8",
   "metadata": {},
   "source": [
    "#### Hipótesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106ca8d",
   "metadata": {},
   "source": [
    "#### Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1e28687",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasons_dict = {}\n",
    "with open(\"reasons.txt\", \"r\") as f:\n",
    "    reasons = [line.strip() for line in f.readlines()]\n",
    "    id = 0\n",
    "    for reason in reasons:\n",
    "        reasons_dict[reason] = id\n",
    "        id += 1\n",
    "\n",
    "reasons = sc.broadcast(reasons_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73289f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_inventory_columns(row: Row):\n",
    "    product_id = parse_to_int(row.product_id)\n",
    "    reason_str = \"UNDEFINED\" if row.reason is None else row.reason.strip().upper()\n",
    "    reason = reasons.value[reason_str]\n",
    "    quantity = parse_to_int(row.quantity_change)\n",
    "    return (\n",
    "        product_id,\n",
    "        reason,\n",
    "        0 if quantity is None else quantity,\n",
    "    )\n",
    "    \n",
    "inventoryIdx = {\n",
    "    \"product_id\": 0,\n",
    "    \"reason\": 1,\n",
    "    \"quantity\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a888e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory = sqlContext.read.csv(\n",
    "    'data/inventory_logs.csv',\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "inventoryRDD = inventory.rdd.map(retain_inventory_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a533cd53",
   "metadata": {},
   "source": [
    "#### Resolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e4d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/06 13:02:00 ERROR Executor: Exception in task 0.0 in stage 55.0 (TID 425)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n",
      "    process()\n",
      "    ~~~~~~~^^\n",
      "  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2036, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 131, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_6719/315092948.py\", line 2, in <lambda>\n",
      "KeyError: 'ELECTRONICS'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/10/06 13:02:00 WARN TaskSetManager: Lost task 0.0 in stage 55.0 (TID 425) (LENOVOID-ubuntu.fibertel.com.ar executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n",
      "    process()\n",
      "    ~~~~~~~^^\n",
      "  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2036, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 131, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_6719/315092948.py\", line 2, in <lambda>\n",
      "KeyError: 'ELECTRONICS'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n",
      "\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n",
      "\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n",
      "\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n",
      "\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "25/10/06 13:02:00 ERROR TaskSetManager: Task 0 in stage 55.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 55.0 failed 1 times, most recent failure: Lost task 0.0 in stage 55.0 (TID 425) (LENOVOID-ubuntu.fibertel.com.ar executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n    process()\n    ~~~~~~~^^\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2036, in process\n    serializer.dump_stream(out_iter, outfile)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 131, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6719/315092948.py\", line 2, in <lambda>\nKeyError: 'ELECTRONICS'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor87.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n    process()\n    ~~~~~~~^^\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2036, in process\n    serializer.dump_stream(out_iter, outfile)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 131, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6719/315092948.py\", line 2, in <lambda>\nKeyError: 'ELECTRONICS'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m electronics_categories_id = categoriesRDD.filter(\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: row[categoriesIdx[\u001b[33m\"\u001b[39m\u001b[33mparent\u001b[39m\u001b[33m\"\u001b[39m]] == reasons.value[\u001b[33m\"\u001b[39m\u001b[33mELECTRONICS\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m ).map(\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: row[categoriesIdx[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m ).collect()\n\u001b[32m      7\u001b[39m electronics_products_ids = productsRDD.filter(\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: (\n\u001b[32m      9\u001b[39m         row[productsIdx[\u001b[33m\"\u001b[39m\u001b[33mcategory_id\u001b[39m\u001b[33m\"\u001b[39m]] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m row: row[productsIdx[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     14\u001b[39m ).collect()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/pyspark/core/rdd.py:1700\u001b[39m, in \u001b[36mRDD.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m.context):\n\u001b[32m   1699\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m     sock_info = \u001b[38;5;28mself\u001b[39m.ctx._jvm.PythonRDD.collectAndServe(\u001b[38;5;28mself\u001b[39m._jrdd.rdd())\n\u001b[32m   1701\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(*a, **kw)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/py4j/protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 55.0 failed 1 times, most recent failure: Lost task 0.0 in stage 55.0 (TID 425) (LENOVOID-ubuntu.fibertel.com.ar executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n    process()\n    ~~~~~~~^^\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2036, in process\n    serializer.dump_stream(out_iter, outfile)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 131, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6719/315092948.py\", line 2, in <lambda>\nKeyError: 'ELECTRONICS'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:203)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor87.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2044, in main\n    process()\n    ~~~~~~~^^\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 2036, in process\n    serializer.dump_stream(out_iter, outfile)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/pat/miniconda3/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 131, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_6719/315092948.py\", line 2, in <lambda>\nKeyError: 'ELECTRONICS'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:581)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:940)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "electronics_categories_id = categoriesRDD.filter(\n",
    "    lambda row: row[categoriesIdx[\"parent\"]] == \"ELECTRONICS\"\n",
    ").map(\n",
    "    lambda row: row[categoriesIdx[\"id\"]]\n",
    ").collect()\n",
    "\n",
    "electronics_products_ids = productsRDD.filter(\n",
    "    lambda row: (\n",
    "        row[productsIdx[\"category_id\"]] is not None\n",
    "        and row[productsIdx[\"category_id\"]] in electronics_categories_id\n",
    "    )\n",
    ").map(\n",
    "    lambda row: row[productsIdx[\"id\"]]\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a73c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_movements = inventoryRDD.filter(\n",
    "    lambda row: row[inventoryIdx[\"reason\"]] == reasons.value[\"DAMAGE\"]\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_electronics_movements_by_product = damaged_movements \\\n",
    ".filter(lambda row: row[inventoryIdx[\"product_id\"]] in electronics_products_ids) \\\n",
    ".map(\n",
    "    lambda row:\n",
    "        (row[inventoryIdx[\"product_id\"]], (1, row[inventoryIdx[\"quantity\"]]))  # (product_id, (total, quantity_change))\n",
    ").reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798963c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/06 12:56:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , log_id, product_id, movement_type, quantity_change, reason, timestamp, reference_id, notes\n",
      " Schema: _c0, log_id, product_id, movement_type, quantity_change, reason, timestamp, reference_id, notes\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/pat/Documents/GitHub/datos-tp2/data/inventory_logs.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_damaged_products = damaged_electronics_movements_by_product \\\n",
    "    .takeOrdered(3, key=lambda x: -x[1][0])\n",
    "\n",
    "most_damaged_products_ids = [prod[0] for prod in most_damaged_products]\n",
    "\n",
    "most_damaged_products_names = productsRDD.filter(\n",
    "    lambda row: row[productsIdx[\"id\"]] in most_damaged_products_ids\n",
    ").map(\n",
    "    lambda row: (row[productsIdx[\"id\"]], row[productsIdx[\"name\"]])\n",
    ").collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "damaged_movements_quantity_mean = damaged_movements.map(\n",
    "    lambda row: row[inventoryIdx[\"quantity\"]]\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f1a28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 3 productos ELECTRONICS con más movimientos por daños son:\n",
      "ID\tTotal Movs.\tCambio tot. en Cant.\tNombre\n",
      "909144\t4\t\t349\t\t\tSelf-enabling discrete open system\n",
      "986689\t4\t\t327\t\t\tUNDEFINED\n",
      "969306\t4\t\t-647\t\t\tPre-emptive zero tolerance encryption\n",
      "\n",
      "El promedio de cambios en la cantidad para los movimientos dañados es de -0.00 unidades.\n"
     ]
    }
   ],
   "source": [
    "print(\"Los 3 productos ELECTRONICS con más movimientos por daños son:\")\n",
    "print(\"ID\\tTotal Movs.\\tCambio tot. en Cant.\\tNombre\")\n",
    "for prod in most_damaged_products:\n",
    "    product_id = prod[0]\n",
    "    total_damages = prod[1][0]\n",
    "    quantity_change = prod[1][1]\n",
    "    product_name = most_damaged_products_names[product_id].strip()\n",
    "    print(f\"{product_id}\\t{total_damages}\\t\\t{quantity_change}\\t\\t\\t{product_name}\")\n",
    "    \n",
    "print(f\"\\nEl promedio de cambios en la cantidad para los movimientos dañados es de {damaged_movements_quantity_mean:.2f} unidades.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
